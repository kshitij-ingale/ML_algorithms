{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assumptions\n",
    "\n",
    "- Linear relation: Linear regression assumes linear relationship between independent variables (input features) and dependent variables\n",
    "- Homoscesdasticity: The variance for different values of response variable should be constant and independent of predictor values\n",
    "- Normality of values: Linear regression assumes normality of target outcome given the features (This assumption can be leveraged to obtain confidence intervals on determined weights)\n",
    "- Independence of errors: Errors of response variables are assumed to be uncorrelated. This also implies that there should be little or no autocorrelation \n",
    "- Lack of perfect multicollinearity: Multicollinearity occurs when independent variables (features) are highly correlated with each other. This leads to errors in estimated weights and also weights can no longer be interpreted as feature importance in this scenario\n",
    "- Weak exogeneity: Predictor values are assumed to be independent of measurement errors (This assumption is violated in many realistic settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters estimation\n",
    "\n",
    "The parameters corresponding to linear regression can be estimated by least squares error. The closed form solution can be obtained by finding paramter values when least squares errors is minimized or when partial derivatives of the error with respect to parameters are zero.\n",
    "$$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "However, for large datasets, the matrix inversion can be computationally expensive. For such cases, matrix factorization techniques like SVD, QR factorization can be used. An alternative to estimate parameter values can be using gradient descent with mean squared error as cost function.\n",
    "\n",
    "Another approach to look at parameter estimation can be using Maximum Likelihood Estimate of parameters. The residuals for response variables can be modeled as normal distribution to obtain the likelihood function using p.d.f. for gaussian distribution. If we compute the average log likelihood for this likelihood function of normally distributed residuals, the expression for parameter values is similar to the one obtained earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider boston housing dataset from sklearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "features, labels = boston.data, boston.target\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=0.2, random_state = 42)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to find parameters\n",
    "mat_xtx = np.dot(train_x.T, train_x)\n",
    "# Find pseudo inverse with SVD using np.linalg.pinv()\n",
    "mat_xtx_inv = np.linalg.pinv(mat_xtx)\n",
    "mat_xty = np.dot(train_x.T, train_y)\n",
    "parameters = np.dot(mat_xtx_inv, mat_xty)\n",
    "\n",
    "# Evaluate model performance\n",
    "predictions = np.dot(test_x,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "The quality of fit for the model can be evaluated using R-squared, however, it is preferred to use adjusted R-squared since, R-squared increases even if features not contributing to prediction at all, are added to model as indicated [here](https://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6039472927932792"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(actual, predictions):\n",
    "    mean_value = np.sum(actual) / actual.shape[0]\n",
    "    SST = np.sum((actual - mean_value) ** 2)\n",
    "    SSR = np.sum((actual - predictions) ** 2)\n",
    "    R2 = 1 - (SSR/SST)\n",
    "    return R2\n",
    "evaluate_model(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6687594935356307"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sklearn linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(rand).fit(train_x, train_y)\n",
    "reg.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
